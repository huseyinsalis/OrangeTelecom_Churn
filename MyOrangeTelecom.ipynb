{"cells":[{"cell_type":"code","source":["from pyspark.sql.types import *\n\nschema = StructType([\n    StructField(\"state_code\", StringType(), True),\n    StructField(\"account_length\", IntegerType(), True),\n    StructField(\"area_code\", StringType(), True),\n    StructField(\"international_plan\", StringType(), True),\n    StructField(\"voice_mail_plan\", StringType(), True),\n    StructField(\"num_voice_mail\", DoubleType(), True),\n    StructField(\"total_day_mins\", DoubleType(), True),\n    StructField(\"total_day_calls\", DoubleType(), True),\n    StructField(\"total_day_charge\", DoubleType(), True),\n    StructField(\"total_evening_mins\", DoubleType(), True),\n    StructField(\"total_evening_calls\", DoubleType(), True),\n    StructField(\"total_evening_charge\", DoubleType(), True),\n    StructField(\"total_night_mins\", DoubleType(), True),\n    StructField(\"total_night_calls\", DoubleType(), True),\n    StructField(\"total_night_charge\", DoubleType(), True),\n    StructField(\"total_international_mins\", DoubleType(), True),\n    StructField(\"total_international_calls\", DoubleType(), True),\n    StructField(\"total_international_charge\", DoubleType(), True),\n    StructField(\"total_international_num_calls\", DoubleType(), True),\n    StructField(\"churn\", StringType(), True)\n])\n\nfileName = \"dbfs:/FileStore/tables/orangeTelecom/churn_bigml_80.csv\"\n\ntrainSet = (spark.read          # Our DataFrameReader\n  .option(\"header\", \"true\")      # Let Spark know we have a header\n  .option(\"inferSchema\", \"false\") # Infering the schema (it is a small dataset)\n  .format(\"com.databricks.spark.csv\")\n  .csv(fileName, schema=schema, nullValue='NA') # Enforce the Schema \n  .cache()                       # Mark the DataFrame as cached.\n)\n\nfileName = \"dbfs:/FileStore/tables/orangeTelecom/churn_bigml_20.csv\"\n\ntestSet = (spark.read          # Our DataFrameReader\n  .option(\"header\", \"true\")      # Let Spark know we have a header\n  .option(\"inferSchema\", \"false\") # Infering the schema (it is a small dataset)\n  .format(\"com.databricks.spark.csv\")\n  .csv(fileName, schema=schema, nullValue='NA') # Enforce the Schema \n  .cache()                       # Mark the DataFrame as cached.\n)\n\ntrainDF = trainSet.drop(\"account_length\", \"state_code\", \"area_code\", \"voice_mail_plan\", \"total_day_mins\", \"total_evening_mins\", \"total_night_mins\", \"total_international_mins\")\ntrainDF.na.drop()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: DataFrame[international_plan: string, num_voice_mail: double, total_day_calls: double, total_day_charge: double, total_evening_calls: double, total_evening_charge: double, total_night_calls: double, total_night_charge: double, total_international_calls: double, total_international_charge: double, total_international_num_calls: double, churn: string]</div>"]}}],"execution_count":1},{"cell_type":"code","source":["\nmajor_df = trainDF.filter(trainDF.churn == False)\nminor_df = trainDF.filter(trainDF.churn == True)\nratio = major_df.count()/minor_df.count()\nprint(\"ratio: {}\".format(ratio))\n\nminor_df_overampled = minor_df.sample(withReplacement=True, fraction=ratio, seed=1)\nTrainDFbalanced = major_df.unionAll(minor_df_overampled)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">ratio: 5.871134020618556\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["print(TrainDFbalanced.filter(TrainDFbalanced.churn == False).count())\nprint(TrainDFbalanced.filter(TrainDFbalanced.churn == True).count())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">2278\n2221\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["TrainDFbalanced.schema"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: StructType(List(StructField(international_plan,StringType,true),StructField(num_voice_mail,DoubleType,true),StructField(total_day_calls,DoubleType,true),StructField(total_day_charge,DoubleType,true),StructField(total_evening_calls,DoubleType,true),StructField(total_evening_charge,DoubleType,true),StructField(total_night_calls,DoubleType,true),StructField(total_night_charge,DoubleType,true),StructField(total_international_calls,DoubleType,true),StructField(total_international_charge,DoubleType,true),StructField(total_international_num_calls,DoubleType,true),StructField(churn,StringType,true)))</div>"]}}],"execution_count":4},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.classification import LogisticRegression, LinearSVC, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\n\n\n# StringIndexer for categorical columns (OneHotEncoder should be evaluated as well)\nipindexer =  StringIndexer(\n    inputCol=\"international_plan\",\n    outputCol=\"iplanIndex\")\n\nlabelindexer =  StringIndexer(\n    inputCol=\"churn\",\n    outputCol=\"label\")\n\n\nassemblerInputs  = [\n                        \"iplanIndex\",  # Our new categorical features\n                        \"num_voice_mail\", \"total_day_charge\", \n                        \"total_day_calls\", \"total_evening_charge\", \n                        \"total_evening_calls\", \"total_night_charge\", \n                        \"total_night_calls\", \"total_international_charge\", \n                        \"total_international_calls\", \"total_international_num_calls\"]        \n\nvectorAssembler = VectorAssembler(\n  inputCols=assemblerInputs, \n  outputCol=\"features\")\n\n \nlr = ( LogisticRegression()\n        .setFeaturesCol(\"features\")\n        .setLabelCol(\"label\")\n     )\n\nlsvc = ( LinearSVC()\n        .setFeaturesCol(\"features\")\n        .setLabelCol(\"label\")\n     )\n\ndt = ( DecisionTreeClassifier()\n        .setFeaturesCol(\"features\")\n        .setLabelCol(\"label\")\n     )\n\nrf = ( RandomForestClassifier()\n        .setFeaturesCol(\"features\")\n        .setLabelCol(\"label\")\n     )\n\ngbt = ( GBTClassifier()\n        .setFeaturesCol(\"features\")\n        .setLabelCol(\"label\")\n     )\n\npipeline = Pipeline().setStages([\n  ipindexer,  \n  labelindexer,         \n  vectorAssembler,         # assemble the feature vector for all columns\n  rf])\n\n#pipelineModel = pipeline.fit(TrainDFbalanced)\n\n#predictionsDF = pipelineModel.transform(testSet)\n\n#predictionsDF.select(\"churn\", \"label\",\"prediction\", \"features\", \"probability\", \"account_length\", \"area_code\", \"international_plan\").show()\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\nnumFolds = 3\nMaxIter = 5\nnumberoftrees = [10, 50,100]\nmaximumdepth = [10, 20, 30]\nRegParam = [0.1, 0.01] # L2 regularization param, set 1.0 with L1 regularization\nTol=1e-8 # for convergence tolerance for iterative algorithms\nElasticNetParam = [0.0, 0.5, 1.0] #Combination of L1 & L2\n\nevaluator = ( BinaryClassificationEvaluator()\n    .setLabelCol(\"label\")\n    .setRawPredictionCol(\"prediction\"))\n\nparamGrid = ParamGridBuilder()\\\n    .addGrid(rf.numTrees, numberoftrees)\\\n    .addGrid(rf.maxDepth, maximumdepth)\\\n    .build()                                               \n#.addGrid(lr.regParam, RegParam) \n#    .addGrid(lr.elasticNetParam, ElasticNetParam)\n#.addGrid(lr.fitIntercept, [False, True])    \n\ncv = ( CrossValidator()\n    .setEstimator(pipeline)\n    .setEvaluator(evaluator)\n    .setEstimatorParamMaps(paramGrid)\n    .setNumFolds(numFolds))\n\ncvModel = cv.fit(TrainDFbalanced)\npredictions = cvModel.transform(testSet)\naccuracy = evaluator.evaluate(predictions)\nprint(\"Classification accuracy: \", accuracy)\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/ml/util.py:737: UserWarning: Cannot find mlflow module. To enable MLflow logging, install mlflow from PyPI.\n  warnings.warn(_MLflowInstrumentation._NO_MLFLOW_WARNING)\nClassification accuracy:  0.8851030548398969\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator = BinaryClassificationEvaluator()\nprint('Area under ROC Curve', evaluator.evaluate(predictions))\n\n# metrics\nauroc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\nauprc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n\n# format the output \nprint(\"Area under receiver operating characteristic (ROC) - Curve: {:.4f}\".format(auroc))\nprint(\"Area under precision-recall (PR) - Curve: {:.4f}\".format(auprc))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Area under ROC Curve 0.9161023187338977\nArea under receiver operating characteristic (ROC) - Curve: 0.9161\nArea under precision-recall (PR) - Curve: 0.8781\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["# convert into RDD for Spark - MLlib\npredictionAndLabels = predictions \\\n     .select(\"prediction\",\"label\") \\\n     .rdd \\\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["from pyspark.mllib.evaluation import BinaryClassificationMetrics\n\nbcm = BinaryClassificationMetrics(predictionAndLabels)\nprint(\"Area under ROC Curve: {:.4f}\".format(bcm.areaUnderROC))\nprint(\"Area under PR Curve: {:.4f}\".format(bcm.areaUnderPR))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Area under ROC Curve: 0.8851\nArea under PR Curve: 0.8489\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["cvModel.bestModel.extractParamMap()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[11]: {}</div>"]}}],"execution_count":10},{"cell_type":"code","source":["lp = predictions.select(\"label\", \"prediction\")\ncounttotal = predictions.count()\ncorrect = lp.filter(predictions.label == predictions.prediction).count()\n\nwrong = lp.filter((predictions.label != predictions.prediction)).count()\nratioWrong = wrong / counttotal\nratioCorrect = correct / counttotal\n\ntruep = lp.filter(predictions.prediction == 0.0).filter(predictions.label == predictions.prediction).count() / counttotal\n\ntruen = lp.filter(predictions.prediction == 1.0).filter(predictions.label == predictions.prediction).count() / counttotal\n\nfalsep = lp.filter(predictions.prediction == 1.0).filter((predictions.label != predictions.prediction)).count() / counttotal\n\nfalsen = lp.filter(predictions.prediction == 0.0).filter((predictions.label != predictions.prediction)).count() / counttotal\n\nprint(\"Total Count : \", counttotal)\nprint(\"Correct : \",  correct)\nprint(\"Wrong: \",  wrong)\nprint(\"Ratio wrong: \" , ratioWrong)\nprint(\"Ratio correct: \",  ratioCorrect)\nprint(\"Ratio true positive : \",  truep)\nprint(\"Ratio false posiive : \",falsep)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Total Count :  667\nCorrect :  641\nWrong:  26\nRatio wrong:  0.038980509745127435\nRatio correct:  0.9610194902548725\nRatio true positive :  0.8500749625187406\nRatio false posiive :  0.0074962518740629685\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["fileName = \"/tmp/churn/LR_Pipeline\"\ncvModel.bestModel.write().overwrite().save(fileName)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":13}],"metadata":{"name":"MyOrangeTelecom","notebookId":1502485329734581},"nbformat":4,"nbformat_minor":0}
